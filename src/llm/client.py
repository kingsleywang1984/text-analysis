from __future__ import annotations

import time
from dataclasses import dataclass, field
from typing import List, Optional, Protocol, runtime_checkable

import requests

from src.config import AppConfig
from src.llm.prompts import (
    SYSTEM_PRODUCT_ANALYST,
    user_label_cluster,
    user_summarize_cluster_comparison,
    user_summarize_comparison,
)
from src.models.schemas import ClusterLabeling, ComparisonSummary


@runtime_checkable
class LLMClient(Protocol):
    """
    Stable interface that pipeline depends on.

    Any implementation (fake, OpenAI, OpenRouter, vLLM, etc.) can be injected as long
    as it satisfies this contract. This enables maintainability and vendor flexibility.
    """

    def label_cluster(self, theme: str, sentences: List[str]) -> ClusterLabeling:
        ...

    def summarize_comparison(
        self, theme: str, baseline_titles: List[str], comparison_titles: List[str]
    ) -> ComparisonSummary:
        ...

    def summarize_cluster_comparison(
        self,
        *,
        theme: str,
        cluster_title: str,
        sentiment: str,
        baseline_sentences: List[str],
        comparison_sentences: List[str],
    ) -> ComparisonSummary:
        ...


@dataclass
class FakeLLMClient(LLMClient):
    """
    Deterministic fake client for tests (no network).
    """

    def label_cluster(self, theme: str, sentences: List[str]) -> ClusterLabeling:
        return ClusterLabeling(
            title=f"{theme}: {sentences[0][:40]}".strip(),
            key_insights=[
                f"Users mention {theme} related issues.",
                "This summary is generated by FakeLLMClient.",
            ],
        )

    def summarize_comparison(
        self, theme: str, baseline_titles: List[str], comparison_titles: List[str]
    ) -> ComparisonSummary:
        return ComparisonSummary(
            key_similarities=[f"Both sets include {theme}-related feedback."],
            key_differences=["One set contains unique clusters not present in the other."],
        )

    def summarize_cluster_comparison(
        self,
        *,
        theme: str,
        cluster_title: str,
        sentiment: str,
        baseline_sentences: List[str],
        comparison_sentences: List[str],
    ) -> ComparisonSummary:
        # Deterministic, non-network output for tests.
        return ComparisonSummary(
            key_similarities=[f"Both cohorts mention **{cluster_title}**."],
            key_differences=[f"Baseline vs comparison differ in emphasis for **{cluster_title}**."],
        )


@dataclass(slots=True)
class OpenAICompatibleChatClient(LLMClient):
    """
    Client for OpenAI-compatible /v1/chat/completions endpoint.

    Compatible means:
    - POST {base_url}/v1/chat/completions
    - Request JSON supports: model, messages, temperature
    - Response JSON follows OpenAI format: choices[0].message.content
    """

    cfg: AppConfig

    _base_url: str = field(init=False, repr=False)
    _api_key: str = field(init=False, repr=False)
    _model: str = field(init=False, repr=False)
    _timeout_s: float = field(init=False, repr=False)
    _temperature: float = field(init=False, repr=False)
    _max_retries: int = field(init=False, repr=False)
    _session: requests.Session = field(init=False, repr=False)

    def __post_init__(self) -> None:
        if not (self.cfg.llm_base_url and self.cfg.llm_api_key and self.cfg.llm_model):
            raise RuntimeError("LLM_BASE_URL / LLM_API_KEY / LLM_MODEL must be set to use real LLM client")

        self._base_url = self.cfg.llm_base_url.rstrip("/")
        self._api_key = self.cfg.llm_api_key
        self._model = self.cfg.llm_model

        self._timeout_s = float(self.cfg.llm_timeout_seconds)
        self._temperature = float(self.cfg.llm_temperature)
        self._max_retries = int(self.cfg.llm_max_retries)

        self._session = requests.Session()
        self._session.headers.update(
            {
                "Authorization": f"Bearer {self._api_key}",
                "Content-Type": "application/json",
            }
        )

    def _chat_json(self, messages: List[dict]) -> dict:
        url = f"{self._base_url}/v1/chat/completions"
        payload = {
            "model": self._model,
            "messages": messages,
            "temperature": self._temperature,
        }

        last_err: Optional[Exception] = None
        for attempt in range(self._max_retries + 1):
            try:
                resp = self._session.post(url, json=payload, timeout=self._timeout_s)
                resp.raise_for_status()
                return resp.json()
            except (requests.Timeout, requests.ConnectionError, requests.HTTPError, ValueError) as e:
                last_err = e
                if attempt < self._max_retries:
                    time.sleep(0.3 * (attempt + 1))
                    continue
                raise RuntimeError("LLM request failed") from last_err

        raise RuntimeError("LLM request failed") from last_err

    @staticmethod
    def _extract_content(data: dict) -> str:
        try:
            return data["choices"][0]["message"]["content"]
        except Exception as e:
            raise RuntimeError("Unexpected LLM response shape") from e

    def label_cluster(self, theme: str, sentences: List[str]) -> ClusterLabeling:
        prompt = user_label_cluster(theme, sentences)
        data = self._chat_json(
            [
                {"role": "system", "content": SYSTEM_PRODUCT_ANALYST},
                {"role": "user", "content": prompt},
            ]
        )
        content = self._extract_content(data)
        return ClusterLabeling.model_validate_json(content)

    def summarize_comparison(
        self, theme: str, baseline_titles: List[str], comparison_titles: List[str]
    ) -> ComparisonSummary:
        prompt = user_summarize_comparison(theme, baseline_titles, comparison_titles)
        data = self._chat_json(
            [
                {"role": "system", "content": SYSTEM_PRODUCT_ANALYST},
                {"role": "user", "content": prompt},
            ]
        )
        content = self._extract_content(data)
        return ComparisonSummary.model_validate_json(content)

    def summarize_cluster_comparison(
        self,
        *,
        theme: str,
        cluster_title: str,
        sentiment: str,
        baseline_sentences: List[str],
        comparison_sentences: List[str],
    ) -> ComparisonSummary:
        prompt = user_summarize_cluster_comparison(
            theme=theme,
            cluster_title=cluster_title,
            sentiment=sentiment,
            baseline_sentences=baseline_sentences,
            comparison_sentences=comparison_sentences,
        )
        data = self._chat_json(
            [
                {"role": "system", "content": SYSTEM_PRODUCT_ANALYST},
                {"role": "user", "content": prompt},
            ]
        )
        content = self._extract_content(data)
        return ComparisonSummary.model_validate_json(content)
